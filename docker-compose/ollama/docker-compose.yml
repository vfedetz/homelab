# https://gist.github.com/usrbinkat/de44facc683f954bf0cca6c87e2f9f88

services:
  ollama-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: ollama-webui
    restart: unless-stopped
    ports:
      - ${OLLAMA_WEBUI_PORT}:8080
    environment:
      - MODEL_DOWNLOAD_DIR=/models
      - OLLAMA_API_BASE_URL=http://${OLLAMA_IP}:${OLLAMA_PORT}
      - OLLAMA_API_URL=http://${OLLAMA_IP}:${OLLAMA_PORT}
      #- LOG_LEVEL=debug
      #- WEBUI_SECRET_KEY=your_secret_key_here  # Add this to prevent logouts after updates
    volumes:
      - ${BASE_DIR}/ollama-webui/data:/data
      #- /mnt/data/models:/models # used to save models to NFS share instead of locally
      - ${BASE_DIR}/ollama-webui/models:/models
      - ${BASE_DIR}/ollama-webui/webui:/app/backend/data
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "2"
    depends_on:
      - ollama
    extra_hosts:
      - "host.docker.internal:host-gateway"

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    runtime: nvidia
    environment:
      - OLLAMA_MODELS=/models
      - OLLAMA_KEEP_ALIVE="-1" # do not unload model from GPU memory
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      #- LOG_LEVEL=debug
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all
    ports:
      - ${OLLAMA_PORT}:11434
    volumes:
      - ${BASE_DIR}/ollama:/root/.ollama
      #- /mnt/data/models:/models # used to save models to NFS share instead of locally
      - ${BASE_DIR}/ollama/models:/models
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "2"
